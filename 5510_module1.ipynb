{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e785bf541c3c6566",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 1: PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b5f4e240125e48e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Problem 1 - Principal Component Analysis\n",
    "---\n",
    "\n",
    "In this problem you'll be implementing Dimensionality reduction using Principal Component Analysis technique. \n",
    "\n",
    "The gist of PCA Algorithm to compute principal components is follows:\n",
    "- Calculate the covariance matrix X of data points.\n",
    "- Calculate eigenvectors and corresponding eigenvalues.\n",
    "- Sort the eigenvectors according to their eigenvalues in decreasing order.\n",
    "- Choose first k eigenvectors which satisfies target explained variance.\n",
    "- Transform the original data of shape m observations times n features into m observations times k selected features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eea5ece608ed4ac5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The skeleton for the *PCA* class is below. Scroll down to find more information about your tasks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e7782486f935045",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-16T14:51:45.753165Z",
     "start_time": "2025-09-16T14:51:45.246457Z"
    }
   },
   "source": [
    "!pip install pytest"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest in ./.venv-3.11/lib/python3.11/site-packages (8.4.1)\r\n",
      "Requirement already satisfied: iniconfig>=1 in ./.venv-3.11/lib/python3.11/site-packages (from pytest) (2.1.0)\r\n",
      "Requirement already satisfied: packaging>=20 in ./.venv-3.11/lib/python3.11/site-packages (from pytest) (25.0)\r\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in ./.venv-3.11/lib/python3.11/site-packages (from pytest) (1.6.0)\r\n",
      "Requirement already satisfied: pygments>=2.7.2 in ./.venv-3.11/lib/python3.11/site-packages (from pytest) (2.19.2)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:51:47.343537Z",
     "start_time": "2025-09-16T14:51:47.249460Z"
    }
   },
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import pytest\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5874a755dcce55e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-16T17:33:02.866349Z",
     "start_time": "2025-09-16T17:33:02.855762Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, target_explained_variance=None):\n",
    "        \"\"\"\n",
    "        explained_variance: float, the target level of explained variance\n",
    "        \"\"\"\n",
    "        self.target_explained_variance = target_explained_variance\n",
    "        self.feature_size = -1\n",
    "\n",
    "    def standardize(self, X):\n",
    "        \"\"\"\n",
    "        standardize features using standard scaler\n",
    "        :param X: input data with shape m (# of observations) X n (# of features)\n",
    "        :return: standardized features (Hint: use skleanr's StandardScaler. Import any library as needed)\n",
    "        \"\"\"\n",
    "        X = np.array(X) # to ensure we have a numpy array\n",
    "        scaler = StandardScaler()\n",
    "        return scaler.fit_transform(X)\n",
    "        \n",
    "\n",
    "    def compute_mean_vector(self, X_std):\n",
    "        \"\"\"\n",
    "        compute mean vector\n",
    "        :param X_std: transformed data\n",
    "        :return n X 1 matrix: mean vector\n",
    "        \"\"\"\n",
    "        return X_std.mean(axis=0)\n",
    "        \n",
    "\n",
    "    def compute_cov(self, X_std, mean_vec):\n",
    "        \"\"\"\n",
    "        Covariance using mean, (don't use any numpy.cov)\n",
    "        :param X_std:\n",
    "        :param mean_vec:\n",
    "        :return n X n matrix:: covariance matrix\n",
    "        \"\"\"\n",
    "        X_centred = X_std - mean_vec # Centred data X = X - mean\n",
    "        m = X_centred.shape[0] # number of observations\n",
    "        cov_mat = np.dot(X_centred.T, X_centred) / (m-1) # 1/(obs - 1) * X_centred.T * X_centred\n",
    "\n",
    "        return cov_mat\n",
    "        \n",
    "\n",
    "    def compute_eigen_vector(self, cov_mat):\n",
    "        \"\"\"\n",
    "        Eigenvector and eigen values using numpy. Uses numpy's eigenvalue function\n",
    "        :param cov_mat:\n",
    "        :return: (eigen_values, eigen_vector)\n",
    "\n",
    "        An eigenvector of the covariance matrix is a direction along which the data’s variance is purely “along that direction,”\n",
    "        without any coupling into perpendicular directions.\n",
    "\n",
    "        The eigenvalue corresponding to an eigenvector gives the variance of the data when projected onto that eigenvector.\n",
    "        \"\"\"\n",
    "        eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "        # make sure they are sorted in descending order (may need later for calculating eigen_pairs)\n",
    "        idx = np.argsort(eig_vals)[::-1]\n",
    "        eig_vals = eig_vals[idx]\n",
    "        eig_vecs = eig_vecs[:,idx]\n",
    "        return eig_vals, eig_vecs\n",
    "        \n",
    "\n",
    "    def compute_explained_variance(self, eigen_vals):\n",
    "        \"\"\"\n",
    "        sort eigen values and compute explained variance.\n",
    "        explained variance informs the amount of information (variance)\n",
    "        can be attributed to each of  the principal components.\n",
    "        :param eigen_vals:\n",
    "        :return: explained variance.\n",
    "        \"\"\"\n",
    "        eigen_vals = np.sort(eigen_vals)[::-1] # make sure they are sorted in descending order\n",
    "        total_var = np.sum(eigen_vals)\n",
    "        var_exp = eigen_vals / total_var\n",
    "        return var_exp\n",
    "        \n",
    "\n",
    "    def cumulative_sum(self, var_exp):\n",
    "        \"\"\"\n",
    "        return cumulative sum of explained variance.\n",
    "        :param var_exp: explained variance\n",
    "        :return: cumulative explained variance\n",
    "        \"\"\"\n",
    "        return np.cumsum(var_exp)\n",
    "\n",
    "    def compute_weight_matrix(self, eig_pairs, cum_var_exp):\n",
    "        \"\"\"\n",
    "        compute weight matrix of top principal components conditioned on target\n",
    "        explained variance.\n",
    "        (Hint : use cumilative explained variance and target_explained_variance to find\n",
    "        top components)\n",
    "        \n",
    "        :param eig_pairs: list of tuples containing eigenvalues and eigenvectors, \n",
    "        sorted by eigenvalues in descending order (the biggest eigenvalue and corresponding eigenvectors first).\n",
    "        :param cum_var_exp: cumulative expalined variance by features\n",
    "        :return: weight matrix (the shape of the weight matrix is n X k)\n",
    "        \"\"\"\n",
    "        idx = np.searchsorted(cum_var_exp, self.target_explained_variance, side=\"left\")\n",
    "        k = idx + 1 #(checking why we don't need to add one here. Doesn't make sense)\n",
    "        print(f'For this test case we have cumulative variance explained given by: {cum_var_exp}')\n",
    "        print(f'To achieve the target explained variance of {self.target_explained_variance}, we need to use the top {k} principal components.')\n",
    "        top_k_vecs = [vec for _, vec in eig_pairs[:k]]\n",
    "        matrix_w = np.column_stack(top_k_vecs)\n",
    "        print(f'The resulting weight matrix has shape {matrix_w.shape} and values \\n{matrix_w}')\n",
    "        return matrix_w\n",
    "        \n",
    "\n",
    "    def transform_data(self, X_std, matrix_w):\n",
    "        \"\"\"\n",
    "        transform data to subspace using weight matrix\n",
    "        :param X_std: standardized data\n",
    "        :param matrix_w: weight matrix\n",
    "        :return: data in the subspace\n",
    "        \"\"\"\n",
    "        return X_std.dot(matrix_w)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"    \n",
    "        entry point to the transform data to k dimensions\n",
    "        standardize and compute weight matrix to transform data.\n",
    "        The fit functioin returns the transformed features. k is the number of features which cumulative \n",
    "        explained variance ratio meets the target_explained_variance.\n",
    "        :param   m X n dimension: train samples\n",
    "        :return  m X k dimension: subspace data. \n",
    "        \"\"\"\n",
    "        matrix_w = []\n",
    "        self.feature_size = X.shape[1]\n",
    "        X_std = self.standardize(X)\n",
    "        mean_vec = self.compute_mean_vector(X_std)\n",
    "        cov_mat = self.compute_cov(X_std, mean_vec)\n",
    "        eig_vals, eig_vecs = self.compute_eigen_vector(cov_mat)\n",
    "        eig_pairs = list(zip(eig_vals_act, eig_vecs_act.T))\n",
    "        # Example element: (eigenvalue, eigenvector_of_shape_(n,))\n",
    "        var_exp = self.compute_explained_variance(eig_vals)\n",
    "        cum_var_exp = self.cumulative_sum(var_exp)\n",
    "        matrix_w = self.compute_weight_matrix(eig_pairs, cum_var_exp)\n",
    "\n",
    "        print(matrix_w.shape)\n",
    "        return self.transform_data(X_std=X_std, matrix_w=matrix_w)\n"
   ],
   "outputs": [],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T17:07:35.417649Z",
     "start_time": "2025-09-16T17:07:35.412081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(42)\n",
    "X = np.array([[0.39, 1.07, 0.06, 0.79], [-1.15, -0.51, -0.21, -0.7], [-1.36, 0.57, 0.37, 0.09], [0.06, 1.04, 0.99, -1.78]])\n",
    "pca_handler = PCA(target_explained_variance=0.99)\n",
    "\n",
    "X_std_act = pca_handler.standardize(X)\n",
    "\n",
    "X_std_exp = [[ 1.20216033, 0.82525828, -0.54269609, 1.24564656],\n",
    "             [-0.84350476, -1.64660539, -1.14693504, -0.31402854],\n",
    "             [-1.1224591, 0.04302294, 0.15105974, 0.51291329],\n",
    "             [ 0.76380353, 0.77832416, 1.53857139, -1.4445313]]\n",
    "\n",
    "for act, exp in zip(X_std_act, X_std_exp):\n",
    "    assert pytest.approx(act, 0.01) == exp, \"Check Standardize function\"\n"
   ],
   "outputs": [],
   "execution_count": 140
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72c2a70fcc1b5457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[ PART A ]** Your task involves implementing helper functions to compute *mean, covariance, eigenvector and weights*.\n",
    "\n",
    "complete `fit()` to using all helper functions to find reduced dimension data.\n",
    "\n",
    "Run PCA on *fashion mnist dataset* to reduce the dimension of the data.\n",
    "\n",
    "fashion mnist data consists of samples with *784 dimensions*.\n",
    "\n",
    "Report the reduced dimension $k$ for target explained variance of **0.99**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ca6638507fdcbd6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "X_train = pickle.load(open('./data/fashionmnist/train_images.pkl','rb'))\n",
    "y_train = pickle.load(open('./data/fashionmnist/train_image_labels.pkl','rb'))\n",
    "\n",
    "X_train = X_train[:1500]\n",
    "y_train = y_train[:1500]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ccdd941a2845fa3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "pca_handler = PCA(target_explained_variance=0.99)\n",
    "X_train_updated = pca_handler.fit(X_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b7f483253e7756e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sample Testing of implemented functions\n",
    "\n",
    "Use the below cells one by one to test each of your functions implemented above. <br>\n",
    "This should serve handy for debugging sections of your code <br>\n",
    "\n",
    "**Please Note - The hidden tests on which the actual points are awarded are different from these and usually run on a different, more complex dataset**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-69de0b2e5db3b495",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-16T17:07:41.896730Z",
     "start_time": "2025-09-16T17:07:41.893208Z"
    }
   },
   "source": [
    "np.random.seed(42)\n",
    "X = np.array([[0.39, 1.07, 0.06, 0.79], [-1.15, -0.51, -0.21, -0.7], [-1.36, 0.57, 0.37, 0.09], [0.06, 1.04, 0.99, -1.78]])\n",
    "pca_handler = PCA(target_explained_variance=0.99)"
   ],
   "outputs": [],
   "execution_count": 141
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4c373a031a46afda",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-16T17:07:42.351593Z",
     "start_time": "2025-09-16T17:07:42.346229Z"
    }
   },
   "source": [
    "X_std_act = pca_handler.standardize(X)\n",
    "\n",
    "X_std_exp = [[ 1.20216033, 0.82525828, -0.54269609, 1.24564656],\n",
    "             [-0.84350476, -1.64660539, -1.14693504, -0.31402854],\n",
    "             [-1.1224591, 0.04302294, 0.15105974, 0.51291329],\n",
    "             [ 0.76380353, 0.77832416, 1.53857139, -1.4445313]]\n",
    "\n",
    "for act, exp in zip(X_std_act, X_std_exp):\n",
    "    assert pytest.approx(act, 0.01) == exp, \"Check Standardize function\""
   ],
   "outputs": [],
   "execution_count": 142
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-07e377f58487a992",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-16T17:07:42.780025Z",
     "start_time": "2025-09-16T17:07:42.776224Z"
    }
   },
   "source": [
    "mean_vec_act = pca_handler.compute_mean_vector(X_std_act)\n",
    "\n",
    "mean_vec_exp = [5.55111512, 2.77555756, 5.55111512, -5.55111512]\n",
    "\n",
    "mean_vec_act_tmp = mean_vec_act * 1e17\n",
    "\n",
    "assert pytest.approx(mean_vec_act_tmp, 0.1) == mean_vec_exp, \"Check compute_mean_vector function\""
   ],
   "outputs": [],
   "execution_count": 143
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-58c635786e1213c4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-16T17:07:43.172601Z",
     "start_time": "2025-09-16T17:07:43.168867Z"
    }
   },
   "source": [
    "\n",
    "cov_mat_act = pca_handler.compute_cov(X_std_act, mean_vec_act)\n",
    "\n",
    "cov_mat_exp = [[ 1.33333333, 0.97573583, 0.44021511, 0.02776305],\n",
    " [ 0.97573583, 1.33333333, 0.88156376, 0.14760488],\n",
    " [ 0.44021511, 0.88156376, 1.33333333, -0.82029039],\n",
    " [ 0.02776305, 0.14760488, -0.82029039, 1.33333333]]\n",
    "\n",
    "assert pytest.approx(cov_mat_act, 0.01) == cov_mat_exp, \"Check compute_cov function\""
   ],
   "outputs": [],
   "execution_count": 144
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-62ec2c97eac4b335",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-16T17:07:43.598955Z",
     "start_time": "2025-09-16T17:07:43.594607Z"
    }
   },
   "source": [
    "eig_vals_act, eig_vecs_act = pca_handler.compute_eigen_vector(cov_mat_act) \n",
    "\n",
    "eig_vals_exp = [2.96080083e+00, 1.80561744e+00, 5.66915059e-01, 7.86907276e-17]\n",
    "\n",
    "eig_vecs_exp = [[ 0.50989282,  0.38162981,  0.72815056,  0.25330765],\n",
    " [ 0.59707545,  0.33170546, -0.37363029, -0.62759286],\n",
    " [ 0.57599397, -0.37480162, -0.41446394,  0.59663585],\n",
    " [-0.22746684,  0.77708038, -0.3980161,   0.43126337]]\n",
    "\n",
    "assert pytest.approx(eig_vals_act, 0.01) == eig_vals_exp, \"Check compute_eigen_vector function\"\n",
    "\n",
    "for act, exp in zip(eig_vecs_act, eig_vecs_exp):\n",
    "    assert pytest.approx(act, 0.01) == exp, \"Check compute_eigen_vector function\""
   ],
   "outputs": [],
   "execution_count": 145
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-14c3cd051410c833",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-16T17:07:44.482355Z",
     "start_time": "2025-09-16T17:07:44.479158Z"
    }
   },
   "source": [
    "pca_handler.feature_size = X.shape[1]\n",
    "var_exp_act = pca_handler.compute_explained_variance(eig_vals_act) \n",
    "\n",
    "var_exp_exp = [0.5551501556710813, 0.33855327084133857, 0.10629657348758019, 1.475451142706682e-17]\n",
    "\n",
    "assert pytest.approx(var_exp_act, 0.01) == var_exp_exp, \"Check compute_explained_variance function\"\n",
    "\n",
    "cum_var = pca_handler.cumulative_sum(var_exp_act)\n",
    "print(cum_var)\n",
    "idx = np.searchsorted(cum_var, .99, side=\"left\")\n",
    "print(idx)\n",
    "print(cum_var[:idx])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55515016 0.89370343 1.         1.        ]\n",
      "2\n",
      "[0.55515016 0.89370343]\n"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3b5a71e28a40a6cc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-16T17:20:30.603581Z",
     "start_time": "2025-09-16T17:20:30.586471Z"
    }
   },
   "source": [
    "eig_pairs = [(2.9608008302457662, np.array([ 0.50989282,  0.59707545,  0.57599397, -0.22746684])),\n",
    "    (1.8056174444871387, np.array([ 0.38162981,  0.33170546, -0.37480162,  0.77708038])),\n",
    "    (0.5669150586004276, np.array([ 0.72815056, -0.37363029, -0.41446394, -0.3980161 ])),\n",
    "    (7.869072761102302e-17, np.array([ 0.25330765, -0.62759286,  0.59663585,  0.43126337]))\n",
    "]\n",
    "\n",
    "cum_var_exp = [0.55515016, 0.89370343, 1, 1]\n",
    "\n",
    "matrix_w_exp = [[0.50989282, 0.38162981], \n",
    "                [0.59707545, 0.33170546], \n",
    "                [0.57599397, -0.37480162], \n",
    "                [-0.22746684, 0.77708038]]\n",
    "\n",
    "matrix_w_act = pca_handler.compute_weight_matrix(eig_pairs=eig_pairs, cum_var_exp=cum_var_exp)\n",
    "\n",
    "\n",
    "for act, exp in zip(matrix_w_act, matrix_w_exp):\n",
    "    assert pytest.approx(act, 0.001) == exp, \"Check compute_weight_matrix function\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this test case we have cumulative variance explained given by: [0.55515016, 0.89370343, 1, 1]\n",
      "To achieve the target explained variance of 0.99, we need to use the top 3 principal components.\n",
      "The resulting weight matrix has shape (4, 3) and values \n",
      "[[ 0.50989282  0.38162981  0.72815056]\n",
      " [ 0.59707545  0.33170546 -0.37363029]\n",
      " [ 0.57599397 -0.37480162 -0.41446394]\n",
      " [-0.22746684  0.77708038 -0.3980161 ]]\n",
      "4 3\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Check compute_weight_matrix function",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[148], line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(matrix_w_act),\u001B[38;5;28mlen\u001B[39m(matrix_w_act[\u001B[38;5;241m0\u001B[39m]))\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m act, exp \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(matrix_w_act, matrix_w_exp):\n\u001B[0;32m---> 19\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m pytest\u001B[38;5;241m.\u001B[39mapprox(act, \u001B[38;5;241m0.001\u001B[39m) \u001B[38;5;241m==\u001B[39m exp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCheck compute_weight_matrix function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mAssertionError\u001B[0m: Check compute_weight_matrix function"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-abb4fbdac246eac8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Result Comparison with Sklearn\n",
    "\n",
    "The below cells should help you compare the output from your implementation against the sklearn implementation with a similar configuration. This is solely to help you validate your work. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5ff5b5144a624a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Use this cell to verify against the sklearn implementation given in the next cell\n",
    "pca_handler.transform_data(X_std=X_std_act, matrix_w=matrix_w)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eef8d927f60236e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Sklearn implementation to compare your results\n",
    "\n",
    "# import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA as pca1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale data before applying PCA\n",
    "scaling=StandardScaler()\n",
    " \n",
    "# Use fit and transform method\n",
    "# You may change the variable X if needed to verify against a different dataset\n",
    "print(\"Sample data:\", X)\n",
    "scaling.fit(X)\n",
    "Scaled_data=scaling.transform(X)\n",
    "print(\"\\nScaled data:\", Scaled_data)\n",
    " \n",
    "# Set the n_components=3\n",
    "principal=pca1(n_components=2)\n",
    "principal.fit(Scaled_data)\n",
    "x=principal.transform(Scaled_data)\n",
    " \n",
    "# Check the dimensions of data after PCA\n",
    "print(\"\\nTransformed Data\",x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
